{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, sys, math, random, glob, shutil, time, functools, itertools\nfrom pathlib import Path ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers \nfrom tensorflow import keras\nimport tensorflow.signal as tfs \n\nfrom scipy.io import wavfile \nfrom IPython.display import Audio, display","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Environment Constants","metadata":{}},{"cell_type":"code","source":"SEED = 1337\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nSR = 16000\nSEGMENT_SEC = 2.0\nSEGMENT = int(SR * SEGMENT_SEC)\nN_FFT = 1024 \nHOP = 256\nWIN_LENGTH = 1024 \nN_MELS = 128\nPAD_MODE = 'REFLECT'\n\nBATCH_SIZE = 8\nEPOCHS = 15\nSTEPS_PER_EPOCH = 600\nVAL_STEPS = 80\nLEARNING_RATE = 3e-4 \nWARMUP_STEPS = 500\nEMA_DECAY = 0.999\nCHECKPOINT_DIR = '/kaggle/working/denoiser_ckpt'\nEXPORT_DIR = '/kaggle/working/denoiser_export'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(CHECKPOINT_DIR, exist_ok = True)\nos.makedirs(EXPORT_DIR, exist_ok = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Audio Utility IO functions","metadata":{}},{"cell_type":"code","source":"def norm_audio(x):\n    x = np.asarray(x, dtype=np.float32)\n    mx = np.max(np.abs(x)) + 1e-9\n    return x / mx","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_wav_mono(path, target_sr=SR):\n    sr,y = wavfile.read(path)\n    y = y.astype(np.float32)\n    if y.ndim == 2:\n        y = y.mean(axis=1)\n    if sr!= target_sr:\n        y = tf.audio.resample(y, sr, target_sr).numpy()\n    return norm_audio(y), target_sr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def write_wav(path, y, sr=SR):\n    y = np.asarray(y, dtype=np.float32)\n    y = (y/(np.max(np.abs(y)) + 1e-9)*0.99)\n    # scaling up from [-1, 1] to 32767\n    wavfile.write(path, sr, (y*32767.0).astype(np.int16))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Signal Transforms","metadata":{}},{"cell_type":"code","source":"# performs short time fourier transform \n# outputs 2d array of complex numbers\ndef stft(sig):\n    return tfs.stft(\n        sig, \n        frame_length=WIN_LENGTH, # how many samples to look at once\n        frame_step=HOP, # how much to hop forward, in our case 1024 - 256 samples will be overlapped\n        fft_length=N_FFT # how many frequency bins result from each analysis\n        window_fn=tf.signal.hann_window # smooths edges to avoid sharp transitions\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# converts time frequency complex representation to time domain audio signal\ndef istft(stft_c, length):\n    return tfs.inverse_stft(\n        stft_c,\n        frame_length=WIN_LENGTH,\n        frame_step=HOP,\n        window_fn=tf.signal.hann_window,\n        output_length=length\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def complex_mag(stft_c):\n    return tf.abs(stft_c)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eps():\n    return 1e-8","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# goes from linear resolution to mel_resolution\n# lower frequency bins are spaced close together (small pitch differences noticable to humans)\nMEL_FILTER = tfs.linear_to_mel_weight_matrix(\n    num_mel_bins=N_MELS,\n    num_spectogram_bins=N_FFT/2 + 1, # linear frequency bins from STFT input\n    sample_rate=SR,\n    lower_edge_hertz=0.0,\n    upper_edge_hertz=SR/2 # Nyquist frequency (half the sample rate)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization functions","metadata":{}},{"cell_type":"code","source":"def plot_waveforms(noisy, clean=None, enhanced=None, sr=SR, title='Waveforms'):\n    plt.figure(figsize=(12, 3))\n    t = np.arange(len(noisy))/sr\n    plt.plot(t, noisy, label='Noisy', linewidth=0.8)\n    if clean is not None:\n        plt.plot(t[:len(clean)], clean, label='Clean', alpha=0.7, linewidth=0.8)\n    if enchanced is not None:\n        plt.plot(t[:len(enhanced)], enhanced, label='Enchanced', alpha=0.9, linewidht=0.8)\n    plt.title(title)\n    plt.xlabel('Time [s]')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def spec_db(mag):\n    return 20.0 * np.log10(np.maximum(mag, 1e-8))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_spectrograms(noisy_mag, clean_mag=None, enhanced_mag=None, sr=SR, title=\"Spectrograms\"):\n    fig, axs = plt.subplots(1, 3 if (clean_mag is not None and enhanced_mag is not None) else 1, figsize=(15, 4))\n    if not isinstance(axs, np.ndarray):\n        axs = np.array([axs])\n    im0 = axs[0].imshow(spec_db(noisy_mag).T, origin=\"lower\", aspect=\"auto\", \n                        extent=[0, noisy_mag.shape[0]*HOP/sr, 0, sr/2])\n    axs[0].set_title(\"Noisy | dB\")\n    axs[0].set_xlabel(\"Time [s]\"); axs[0].set_ylabel(\"Freq [Hz]\")\n    fig.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)\n\n    if clean_mag is not None and enhanced_mag is not None:\n        im1 = axs[1].imshow(spec_db(clean_mag).T, origin=\"lower\", aspect=\"auto\",\n                            extent=[0, clean_mag.shape[0]*HOP/sr, 0, sr/2])\n        axs[1].set_title(\"Clean | dB\"); axs[1].set_xlabel(\"Time [s]\")\n        fig.colorbar(im1, ax=axs[1], fraction=0.046, pad=0.04)\n\n        im2 = axs[2].imshow(spec_db(enhanced_mag).T, origin=\"lower\", aspect=\"auto\",\n                            extent=[0, enhanced_mag.shape[0]*HOP/sr, 0, sr/2])\n        axs[2].set_title(\"Enhanced | dB\"); axs[2].set_xlabel(\"Time [s]\")\n        fig.colorbar(im2, ax=axs[2], fraction=0.046, pad=0.04)\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_mask(mask, title=\"Predicted Mask\"):\n    plt.figure(figsize=(6,4))\n    plt.imshow(mask.T, origin=\"lower\", aspect=\"auto\",\n               extent=[0, mask.shape[0]*HOP/SR, 0, SR/2])\n    plt.title(title); plt.xlabel(\"Time [s]\"); plt.ylabel(\"Freq [Hz]\")\n    plt.colorbar(fraction=0.046, pad=0.04)\n    plt.tight_layout(); plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Synthetic Noise Generator (Clean and Noisy dataset)","metadata":{}},{"cell_type":"code","source":"def gen_tone(duration, sr=SR):\n    t = np.linspace(0, duration, int(sr*duration), endpoint=False)\n    # generating a time array 16k points per sec\n    f0 = np.random.uniform(100, 1000) # base frequency\n    y = np.sin(2*np.pi*f0*t) # pure sine wave at frequency f0\n    # blend pure freqency with chirp with 50% probab\n    if np.random.rand() < 0.5:\n        f1 = np.random.uniform(200, 2000)\n        # tone whose frequency changes over time, keeps changing from f0 to f1 linearly\n        chirp = np.sin(2*np.pi*(f0 + (f1-f0)*t/duration)*t)\n        y = 0.6*y + 0.4*chirp\n    env = 0.5*(1-np.cos(2*np.pi*np.minimum(1.0, t/duration)))\n    # smooth cosine curve controlling volume over time, prevents sudden starts or stops\n    return norm_audio(y * env)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gen_noise(duration, sr=SR):\n    n = int(sr*duration)\n    # white noise: energy concentration equal\n    white = np.random.randn(n).astype(np.float32)\n    freqs = np.fft.rfftfreq(n, 1/sr) # 1d array\n    # pink noise: energy concentrate more at lower frequency\n    pink_spec = (np.random.randn(len(freqs))+1j*np.random.randn(len(freqs)))/np.maximum(freqs, 1.0)\n    # random complex numbers to generate noise, frequency below zero stays same\n    pink = np.fft.irfft(pink_spec, n=n).astype(np.float32)\n    # convert back to time domain\n    babble = np.zeros(n, dtype=np.float32)\n    # summing up several tones (3 to 6) to simulate overlapping sounds\n    for _ in range(np.random.randint(3, 7)):\n        babble += gen_tone(duration, sr)\n    babble = babble / (np.max(np.abs(babble)) + 1e-9)\n    mix = 0.5*white/np.max(np.abs(white)+1e-9) + 0.3*pink/np.max(np.abs(pink)+1e-9) + 0.2*babble\n    return norm_audio(mix)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_segment(y, length):\n    if len(y) < length:\n        pad = length - len(y)\n        y = np.pad(y, (0, pad), mode='reflect')\n        return y\n    start = np.random.randint(0, len(y)-length)\n    return y[start:start+length]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mix_clean_noise(clean, noise, snr_db=None):\n    if snr_db is None:\n        snr_db = np.random.uniform(-5, 15)\n    # normalizing both\n    c = clean / (np.std(clean)+1e-9)\n    n = noise / (np.std(noise)+1e-9)\n    # getting rms of both signals\n    rms_c = np.sqrt(np.mean(c**2)+1e-9)\n    rms_n = np.sqrt(np.mean(n**2)+1e-9)\n    target_rms_n = rms_c / (10**(snr_db/20.0))\n    # scaling the noise to get the desired ratio\n    n = n * (target_rms_n / (rms_n + 1e-9))\n    noisy = c + n\n    return norm_audio(noisy), norm_audio(c), norn_audio(n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pipeline","metadata":{}},{"cell_type":"code","source":"def wav_loader_factory(clean_paths, noise_paths):\n    # loader function yields one noisy, clean pair (is iterable)\n    def load_and_mix(_):\n        if clean_paths:\n            cp = random.choice(clean_paths)\n            c, _sr = read_wav_mono(cp, SR)\n        else:\n            c = gen_tone(SEGMENT_SEC)\n        if noise_paths and np.random.rand() < 0.9:\n            npth = random.choice(noise_paths)\n            n, _sr = read_wav_mono(npth, SR)\n        else:\n            n = gen_noise(SEGMENT_SEC + 1.0)\n        c_seg = random_segment(c, SEGMENT)\n        n_seg = random_segment(n, SEGMENT)\n        # noisy will be the model input and clean will be the target\n        noisy, clean, noise = mix_clean_noise(c_seg, n_seg)\n        return noisy.astype(np.float32), clean.astype(float32)\n    return load_and_mix\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example workflow:\n# For an example step size of 600\n# Each call to loader function returns a (noisy, clean) pair, ((32000,), (32000,))\n# Gen functions inside tf_dataset calls the loader function 600 * 8 * 2 = 9600 times\n# For each epoch a fresh pool is generate 9600 new samples\n# The samples are shuffled\n# From this pool, batches of 8 are created, so in total 600 batches of 8 samples are created\n# Train_ds is an iterable object\n# Calling next() on it yields one batch -> ((8, 32000), (8, 32000))\n# You can call the next function 600 times\n\ndef tf_dataset(clean_paths, noise_paths, batch_size, steps):\n    # generator function to call the loader function 2 * required amount times (helps in shuffling)\n    def gen(): # stream\n        loader = wav_loader_factory(clean_paths, noise_paths)\n        for _ in range(steps * batch_size * 2):\n            yield loader(None)\n    # output dimensions\n    output_sig = (tf.TensorSpec(shape=(SEGMENT,), dtype=tf.float32),\n                  tf.TensorSpec(shape=(SEGMENT,), dtype=tf.float32))\n    # reiterable (generates fresh pool for every epoch)\n    ds = tf.data.Dataset.from_generator(gen, output_signature=output_sig)\n    ds = ds.shuffle(8192, reshuffle_each_iteration=True)\n    ds = ds.batch(batch_size, drop_remainder=True)\n    # asynchronously preparing the next batch while the current one is being processed\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n    return ds\n\ntrain_ds = tf_dataset(CLEAN_WAVS, NOISE_WAVS, BATCH_SIZE, STEPS_PER_EPOCH)\nval_ds = tf_dataset(CLEAN_WAVS, NOISE_WAVS, BATCH_SIZE, VAL_STEPS)\n\nnoisy_b, clean_b = next(iter(train_ds))\nprint(\"Batch shapes:\", noisy_b.shape, clean_b.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# layer friendly stft functions \n# input: (Batch, Datapoints): eg (8, 32000) for 2 sec sample\ndef stft_layer(x):\n    X = tf.numpy_function(lambda a: tfs.stft(a, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window).numpy(),\n                         [x], Tout=tf.complex64)\n    X.set_shape([None, None, N_FFT//2 + 1])\n    return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class STFTMagLayer(layers.Layer):\n    def call(self, x):\n        # input : (8, 32000)\n        # output: (8, 122, 513)\n        X = tfs.stft(x, frame_length=WIN_LENGTH, frame_step=HOP, fft_length=N_FFT,\n                    window_fn=tf.signal.hann_window) # outputs array of complex numbers\n        mag = tf.abs(X) # magnitude\n        phase = tf.math.angle(X) # phase \n        # helpful for reconstructing audio in inverse stft\n        return tf.transpose(mag, [0, 1, 2]), tf.transpose(phase, [0, 1, 2]), X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def db_log(x):\n    return tf.math.log(x+ 1e-6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inv_db_log(x):\n    return tf.math.expm1(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def unet_block(x, filters, name, down=True):\n    # downsampling (encoder)\n    # convolution operation, decreases the dimensions and captures only the import features\n    # batch norm in every block to stabalize training\n    if down:\n        x = layers.Conv2D(filters, 3, strides=2, padding='same', name=name+'_conv')(x)\n        x = layers.BatchNormalization(name=name+'_bn')(x)\n        x = layers.Activation('relu', name=name+'_relu')(x)\n        return x\n    # upsampling (decoder)\n    # transpose convolution operation, increases the dimensions, learns to fill in the gaps\n    else:\n        x = layer.Conv2DTranspose(filters, 3, strides=2, padding='same', name=name+'_deconv')(x)\n        x = layers.BatchNormalization(name=name+'_bn')(x)\n        x = layers.Activation('relu', name=name+'_relu')(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convolutions capture local structures like harmocis and noise texture\n# learn how energy is distributed across different frequencies\n\ndef build_unet(n_mels=None):\n    inp = keras.Input(shape=(SEGMENT,), name='audio_in')\n    # size of an individual sample from the batch, batch handled automatically during training\n\n    # fourier transform\n    Xc = tfs.stft(inp, frame_length=WIN_LENGTH, frame_step=HOP, fft_length=N_FFT, window_fn=tf.signla.hann_window)\n    \n    # taking magnitude and phase angle from complex number output\n    mag = tf.abs(Xc) # spectral quality\n    phase = tf.math.angle(Xc) # timing, position within wave\n\n    # adding additional dimension to make it compatible with conv layer\n    M = tf.expand_dims(mag, -1)\n\n    # Encoder block (passing stft output through 5 conv layers)\n    # Reduces resolution through downsampling (lets the neurons see the bigger picture)\n    e1 = layers.Conv2D(32, 3, padding='same', activation='relu')(M)\n    d1 = unet_block(e1, 64, 'down1')\n    d2 = unet_block(d1, 128, 'down2')\n    d3 = unet_block(d2, 256, 'down3')\n    bott = layers.Conv2D(512, 3, padding='same', activation='relu')(d3)\n\n    # Decoder block\n    # each output of upsampling is compared with original version through skip connection\n    u3 = unet_block(bott, 256, 'up3', down=False)\n    u3 = layers.Concatenate()([u3, d2])\n    u2 = unet_block(u3, 128, 'up2', down=False)\n    u2 = layers.Concatenate()([u2, d1])\n    u1 = unet_block(u2, 64, 'up1', down=False)\n    u1 = layers.Concatenate*([u1, e1])\n\n    # soft mask prediction for each time frequency bin of input\n    # soft mask values (0-1), closer to 1 means the part is clear speech and should be preserved\n    out_mask = layers.Conv2D(1, 1, activation='sigmoid', name='mask')(u1)\n    out_mask = tf.squeeze(out_mask, -1)\n\n    # enchanced magnitude after applying the soft mask\n    # after applying the mask we get the predicted clean output which is compared with target output\n    enh_mag = out_mask * mag \n    real = enh_mag * tf.cos(phase)\n    imag = enh_mag * tf.sin(phase)\n    enh_complex = tf.complex(real, imag)\n    enh_audio = tfs.inverse_stft(enh_complex, frame_length=WINDOW_LENGTH, frame_step=HOP, window_fn=tf.signal.hann_window, output_length=SEGMENT)\n\n    return keras.Model(inp, outputs=[enh_audio, out_mask, mag], name='UNet_Denoiser')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = build_net()\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss Functions","metadata":{}},{"cell_type":"code","source":"# Average absolute difference between actual and predicted\ndef l1_mag_loss(true_audio, pred_audio):\n    Y = tfs.stft(true_audio, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)\n    P = tfs.stft(pred_audio, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)\n    return tf.reduce_mean(tf.abs(tf.abs(Y)-tf.abs(P)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# difference relative to clean spectrogram, focus on matching overall energy dist\n# penalizing relative error rather than absolute\ndef spectral_convergance(true_audio, pred_audio):\n    Y = tfs.stft(true_audio, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)\n    P = tfs.stft(pred_audio, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)\n    num = tf.norm(tf.abs(Y)-tf.abs(P), ord='fro')\n    den = tf.norm(tf.abs(Y), ord='fro') + eps()\n    return num / den","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Multi Resolution STFT loss\n# compares absolute difference for different window and hop sizes\n\ndef mrstft_loss(true_audio, pred_audio):\n    cfgs = [\n        (1024, 526), (512, 128), (2048, 512)\n    ]\n    loss = 0.0\n    for nfft, hop in cfgs:\n        Y = tfs.stft(true_audio, hop, nfft, window_fn=tf.signal.hann_window)\n        P = tfs.stft(pred_audio, hop, nfft, window_fn=tf.signal.hann_window)\n        loss += tf.reducde_mean(tf.abs(tf.abs(Y)-tf.abs(P)))\n    return loss / len(cfgs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# point -> as long as the waveform shape matches, amplitude mismatches are ignored\n# penalize unwanted signal content that cannot be explained by rescaling clean signal\ndef si_sdr(true_audio, pred_audio):\n    x = true_audio\n    s = pred_audio\n    x_zm = x - tf.reduce_mean(x, axis=-1, keepdims=True)\n    s_zm = s - tf.reduce_mean(x, axis=-1, keepdims=True)\n    proj = tf.reduce_sum(s_zm * x_zm, axis=-1) / (tf.reduce_sum(x_zm**2, axis=-1, keepdims=True) + eps()) * x_zm\n    e = s_zm - proj\n    si_sdr_val = 10 * tf.math.log((tf.reduce_sum(proj**2, axis=-1)+eps()) / (tf.reduce_sum(e**2, axis=-1)+eps()))\n    return si_sdr_val\n\n# Scale invariant signal to distortion ratio loss -> Quality of reconstructed waveform in time domain\ndef si_sdr_loss(true_audio, pred_audio):\n    return -tf.reduce_mean(si_sdr(true_audio, pred_audio))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# combined loss\ndef total_loss(true_audio, pred_audio):\n    return (0.5 * l1_mag_loss(true_audio, pred_audio) +\n            0.2 * spectral_convergence(true_audio, pred_audio) +\n            0.2 * mrstft_loss(true_audio, pred_audio) +\n            0.1 * si_sdr_loss(true_audio, pred_audio))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimizer","metadata":{}},{"cell_type":"code","source":"# Two phase learning rate scheduling \n# First gradually increase the learning rate (warmup) and then smoothly decrease it throughout training\n# The decrease follows a smooth cosine curve\n\nclass WarmupCosine(keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, base_lr, warmup_steps, total_steps):\n        super().__init__()\n        self.base_lr = base_lr # starting learning rate\n        self.warmup = warmup_steps # increase upto warmup steps\n        self.total = total_steps\n\n    def __call__(self, step): # gives learning rate at given step\n        step = tf.cast(step, tf.float32)\n        warm = tf.cast(self.warmup, tf.float32)\n        total = tf.cast(self.total, tf.float32)\n        lr = tf.where(\n            step < warm,\n            self.base_lr * (step / tf.maximum(1.0, warm)),\n            0.5 * self.base_lr * (1 + tf.cos(np.pi * (step - warm) / tf.maximum(1.0, (total-warm))))\n        )\n        return lr\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\nlr_schedule = WarmupCosine(LEARNING_RATE, WARMUP_STEPS, TOTAL_STEPS)\nopt = keras.optimizer.Adam(learning_rate=lr_schedule)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exponential Moving Average","metadata":{}},{"cell_type":"code","source":"# Exponential Moving Average for better generalization and stable performance\n# Weighted average of past weights is stored with recent weights having higher weight\n# Hence, to get the new weight value not just the latest weights are used\n\nclass EMA:\n    def __init__(self, model, decay=EMA_DECAY):\n        self.decay = decay # how weight to be given to the older stored average weights\n        self.shadow = [tf.identity(w) for w in model.weights]\n\n    def update(self, model):\n        # new_weight = old_weight * decay + current_model_weight * (1 - decay)\n        for i, w in enumerate(model.weights):\n            self.shadow[i].assign(self.decay * self.shadow[i] + (1.0 - self.decay) * w)\n        # change is more smooth without jumping around\n\n    def apply_to(self, model):\n        self.backup = [tf.identity(w) for w in model.weights] # temporary backup\n        for w, s in zip(model.weights, self.shadow):\n            w.assign(t)\n\n    def restore(self, model):\n        # reverts model back original weights before apply_to was called\n        # allowing toggling weight type during training and evaluation\n        for w, b in zip(model.weights, self.backup):\n            w.assign(b)\n        self.backup = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ema = EMA(model, EMA_DECAY)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom Training Loop with Custom loss and EMA","metadata":{}},{"cell_type":"code","source":"train_loss = keras.metrics.Mean()\nval_loss_metric = keras.metrics.Mean()\ntrain_si_sdr_metric = keras.metrics.Mean()\nval_si_sdr_metric = keras.metrics.Mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function \ndef train_step(noisy, clean):\n    with tf.GradientTape() as tape:\n        enhanced_audio, mask, mag = model(noisy, training=True)\n        loss = total_loss(clean, enhanced_audio)\n        if tf.keras.mixed_precision.global_policy().compute_dtype == 'float16':\n            loss = tf.cast(loss, tf.float32)\n    # calculating the gradient of the loss wrt to all the weights \n    grads = tape.gradient(loss, model.trainable_variables)\n    # applying the gradients \n    opt.apply_gradients(zip(grads, model.trainable_variables))\n    # updating the ema weights \n    ema.update(model)\n    # updating the metric states\n    train_loss_metric.update_state(loss)\n    train_si_sdr_metric.update_state(si_sdr(clean, enhanced_audio))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function \ndef val_step(noisy, clean):\n    enhanced_audio, mask, mag = model(noisy, training=False)\n    loss = total_loss(clean, enhanced_audio)\n    val_loss_metric.update_state(loss)\n    val_si_sdr_metric.update_state(si_sdr(clean, enhanced_audio))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"history = {'loss': [], 'val_loss': [], 'si_sdr': [], 'val_si_sdr': []}\nglobal_step = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(1, EPOCHS+1):\n    print(f'\\nEpoch {epoch}/{EPOCHS}')\n    # resetting all the metric states to zero at the start of each epoch\n    train_loss_metric.reset_states()\n    val_loss_metric.reset_states()\n    train_si_sdr_metric.reset_states()\n    val_si_sdr_metric.reset_states()\n\n    for step, (noisy, clean) in enumerate(train_ds.take(STEPS_PER_EPOCH), start=1):\n        # train step\n        train_step(noisy, clean)\n        global_step += 1\n        if step % 100 == 0:\n            print(f\"  step {step}/{STEPS_PER_EPOCH}  lr={opt.lr(global_step).numpy():.6f}  \"\n                  f\"loss={train_loss_metric.result().numpy():.4f}  \"\n                  f\"SI-SDR={train_si_sdr_metric.result().numpy():.2f}dB\")\n    # temporarily apply ema weights during the validation phase\n    ema.apply_to(model)\n    for step, (noisy, clean) in enumerate(val_ds.take(VAL_STEPS), start=1):\n        val_step(noisy, clean)\n    # restore the old weights to resume training \n    ema.restore(model)\n\n    tr_loss = float(train_loss_metric.result().numpy())\n    va_loss = float(val_loss_metric.result().numpy())\n    tr_sdr = float(train_si_sdr_metric.result().numpy())\n    va_sdr = float(val_si_sdr_metric.result().numpy())\n\n    history['loss'].append(tr_loss)\n    history['val_loss'].append(va_loss)\n    history['si_sdr'].append(tr_sdr)\n    history['val_si_sdr'].append(va_sdr)\n\n    # save checkpoint \n    ema.apply_to(model)\n    model.save_weights(os.path.join(CHECKPOINT_DIR, f'epoch{epoch:0.2d}.weights.h5'))\n    ema.restore(model)\n\n    print(f\"Epoch {epoch} done. train_loss={tr_loss:.4f} val_loss={va_loss:.4f} \"\n          f\"train_SI-SDR={tr_sdr:.2f}dB val_SI-SDR={va_sdr:.2f}dB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,3))\nplt.plot(history[\"loss\"], label=\"train loss\")\nplt.plot(history[\"val_loss\"], label=\"val loss\")\nplt.title(\"Loss Curves\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.tight_layout(); plt.show()\n\nplt.figure(figsize=(10,3))\nplt.plot(history[\"si_sdr\"], label=\"train SI-SDR (dB)\")\nplt.plot(history[\"val_si_sdr\"], label=\"val SI-SDR (dB)\")\nplt.title(\"SI-SDR Curves\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"dB\"); plt.legend(); plt.tight_layout(); plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Qualitative Check","metadata":{}},{"cell_type":"code","source":"ema.apply_to(model)\n\nnoisy_b, clean_b = next(iter(val_ds))\nenh_b, mask_b, mag_b = model(noisy_b, training=False)\n\nidx = 0\nnoisy = noisy_b[idx].numpy()\nclean = clean_b[idx].numpy()\nenh = enh_b[idx].numpy()\n\nprint(\"Playing audio (Noisy -> Enhanced -> Clean)\")\ndisplay(Audio(noisy, rate=SR))\ndisplay(Audio(enh, rate=SR))\ndisplay(Audio(clean, rate=SR))\n\nN = tfs.stft(noisy, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)\nC = tfs.stft(clean, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)\nE = tfs.stft(enh,   WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)\nplot_spectrograms(np.abs(N).numpy(), np.abs(C).numpy(), np.abs(E).numpy(),\n                  sr=SR, title=\"Noisy / Clean / Enhanced (Mag dB)\")\nplot_mask(mask_b[idx].numpy(), title=\"Predicted Soft Mask\")\nplot_waveforms(noisy, clean, enh, sr=SR, title=\"Waveforms\")\n\nema.restore(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ema.apply_to(model)\n\nclass InferenceWrapper(keras.Model):\n    def __init__(self, base):\n        super().__init__()\n        self.base = base\n    @tf.function(input_signature=[tf.TensorSpec([None], tf.float32)])\n    def denoise(self, audio):\n        audio = tf.expand_dims(audio, 0)\n        enh, _mask, _mag = self.base(audio, training=False)\n        return tf.squeeze(enh, 0)\n\ninfer_model = InferenceWrapper(model)\ntf.save_model.save(infer_model, EXPORT_DIR)\nprint(\"Exported to: \", EXPORT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denoise_file(wav_path, out_path=None, show=True):\n    y, _ = read_wav_mono(wav_path, SR)\n    # pad/trim to multiple of hop via center-padding for a nicer output (optional)\n    T = len(y)\n    pad = ( (math.ceil(T / HOP) * HOP) - T )\n    y_pad = np.pad(y, (0, pad), mode='reflect').astype(np.float32)\n\n    ema.apply_to(model)\n    enh, mask, _ = model(tf.convert_to_tensor(y_pad[None, ...]), training=False)\n    ema.restore(model)\n\n    enh = enh.numpy()[0][:T]\n\n    if out_path:\n        write_wav(out_path, enh, SR)\n\n    if show:\n        print(\"Playing (Noisy -> Enhanced)\")\n        display(Audio(y, rate=SR))\n        display(Audio(enh, rate=SR))\n        N = tfs.stft(y, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)\n        E = tfs.stft(enh, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)\n        plot_spectrograms(np.abs(N).numpy(), None, None, sr=SR, title=\"Noisy Spectrogram (dB)\")\n        plot_spectrograms(np.abs(E).numpy(), None, None, sr=SR, title=\"Enhanced Spectrogram (dB)\")\n        plot_waveforms(y, None, enh, sr=SR, title=\"Inference Waveforms\")\n    return enh\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nReady! Use: denoise_file('/kaggle/input/your.wav', out_path='/kaggle/working/clean.wav')\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"demo_clean = gen_tone(2.0)\ndemo_noise = gen_noise(2.0)\ndemo_noisy, demo_clean, _ = mix_clean_noise(demo_clean, demo_noise, snr_db=0.0)\n_ = denoise_file(wav_path=None if True else \"\", out_path=None)  # no-op to show usage\nprint(\"Demo playback:\")\ndisplay(Audio(demo_noisy, rate=SR))\nema.apply_to(model)\ndemo_enh, _, _ = model(tf.convert_to_tensor(demo_noisy[None, ...]), training=False)\nema.restore(model)\ndemo_enh = demo_enh.numpy()[0]\ndisplay(Audio(demo_enh, rate=SR))\nplot_spectrograms(np.abs(tfs.stft(demo_noisy, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)).numpy(),\n                  None, None, title=\"Demo Noisy Spec (dB)\")\nplot_spectrograms(np.abs(tfs.stft(demo_enh, WIN_LENGTH, HOP, N_FFT, window_fn=tf.signal.hann_window)).numpy(),\n                  None, None, title=\"Demo Enhanced Spec (dB)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}